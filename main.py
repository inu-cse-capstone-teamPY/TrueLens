#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ì‚¬ì‹¤ ê²€ì¦ íŒŒì´í”„ë¼ì¸ (SerpAPI + OpenAI Responses API)
=================================================================

ì´ ëª¨ë“ˆì€ **ì…ë ¥ í…ìŠ¤íŠ¸ â†’ (ì‚¬ì‹¤ ì£¼ì¥ ì¶”ì¶œ) â†’ (ì›¹ ê·¼ê±° ìˆ˜ì§‘) â†’ (LLM ê·¼ê±°íŒì •)** ì˜
ì²´ì¸ì„ ìˆ˜í–‰í•´, ê° ì£¼ì¥ì— ëŒ€í•´ **ì‹ ë¢°ë„ ì ìˆ˜**ë¥¼ ê³„ì‚°í•˜ê³  ì½˜ì†”ìš© ë¦¬í¬íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤.

1) í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— .env íŒŒì¼ ìƒì„± (ìµœì†Œ):

    OPENAI_API_KEY=sk-...
    SERPAPI_API_KEY=xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx

    ì„ íƒ ì˜µì…˜(ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©):

    FACTCHAIN_MODEL=gpt-5-mini
    FACTCHAIN_MAX_RESULTS=6
    FACTCHAIN_TIMEOUT=20
    FACTCHAIN_HL=ko
    FACTCHAIN_GL=kr

2) ì‹¤í–‰:

    python main.py --text "HTTP/3ëŠ” QUIC ìœ„ì—ì„œ ë™ì‘í•œë‹¤."

3) í˜¹ì€ íŒŒì¼ í•˜ë‹¨ì˜ DEMO_TEXTë¥¼ ìˆ˜ì •í•´ ë°”ë¡œ ì‹¤í–‰.

ì„¤ê³„ ê°œìš”
---------
íŒŒì´í”„ë¼ì¸ ë‹¨ê³„:
1) step1_extract_claims  â€” ì‚¬ì‹¤ ì£¼ì¥ + (ì£¼ì œ ì¹´í…Œê³ ë¦¬) ì¶”ì¶œ
2) step2_collect_evidence â€” GOOGLE CSE APIë¡œ ë²„í‚· ê²€ìƒ‰ í›„ ë³‘í•©/í‹°ì–´ ì •ë ¬
3) step3_evaluate_sources â€” LLMìœ¼ë¡œ supports/refutes/irrelevant íŒì •
4) step4_score â€” íœ´ë¦¬ìŠ¤í‹± + íŒì •/í™•ì‹ ë„ â†’ 0~100 ì‹ ë¢°ì ìˆ˜

"""
from __future__ import annotations

import os
import re
import json
import time
import argparse
import logging
import urllib.parse as urlparse
from dataclasses import dataclass, asdict
from typing import List, Dict, Any, Optional, Tuple
from string import Template

from dotenv import load_dotenv
from openai import OpenAI
from concurrent.futures import ThreadPoolExecutor, as_completed


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë¡œê¹… ì„¤ì •
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logger = logging.getLogger("factchain")
if not logger.handlers:
    _h = logging.StreamHandler()
    _h.setFormatter(logging.Formatter("[%(levelname)s] %(message)s"))
    logger.addHandler(_h)
logger.setLevel(logging.INFO)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í™˜ê²½ì„¤ì •(.env) ê¸°ë°˜ ê¸°ë³¸ê°’ â€” í•„ìš” ì‹œ CLIë¡œ ë®ì–´ì“°ê¸° ê°€ëŠ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
MODEL_DEFAULT = os.getenv("FACTCHAIN_MODEL", "gpt-4o-mini")
MAX_RESULTS = int(os.getenv("FACTCHAIN_MAX_RESULTS", "6"))
TIMEOUT_S = int(os.getenv("FACTCHAIN_TIMEOUT", "20"))
SERPAPI_ENDPOINT = os.getenv("SERPAPI_ENDPOINT", "https://serpapi.com/search.json")
SERP_HL = os.getenv("FACTCHAIN_HL", "ko")   # Google ì–¸ì–´(UI)
SERP_GL = os.getenv("FACTCHAIN_GL", "kr")   # Google ì§€ì—­/êµ­ê°€

# step1ì—ì„œ ì‚¬ìš©í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡
FACT_CATS = ["tech", "science", "policy", "health", "finance", "general", "community"]

# step2ì—ì„œ ì‚¬ìš©í•  ê° ì£¼ì œë³„ ê²€ìƒ‰ ì˜µì…˜
CATEGORY_PRESETS: Dict[str, List[str]] = {
    "tech":      ["scholarly", "government", "news", "general"],
    "science":   ["scholarly", "government", "news"],
    "policy":    ["government", "news", "general", "community"],
    "health":    ["government", "scholarly", "news"],
    "finance":   ["news", "general", "government"],
    "community": ["community", "news", "general"],
    "general":   ["news", "general", "government"],
}


DEFAULT_PRESET = "general"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë„ë©”ì¸ ì‹ ë¢° í‹°ì–´(ì •ê·œì‹) â€” 3: ìµœê³ (ì •ë¶€/êµ­ì œ/í•™ìˆ /í‘œì¤€) / 2: ì–¸ë¡ /ëŒ€ê¸°ì—… ë“± / 1: ê¸°íƒ€
# ì¶”ê°€ ì‹œ ë³´ìˆ˜ì ìœ¼ë¡œ ê´€ë¦¬, ë¦¬ë·° í•„ìˆ˜
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TRUST_TIER_PATTERNS: Dict[int, List[str]] = {
    # 3í‹°ì–´ â€” ì •ë¶€/ê³µê³µ/í•™ìˆ /êµ­ì œ/í•µì‹¬ í‘œì¤€
    3: [
        r"(^|\.)gov$", r"(^|\.)go\.kr$", r"(^|\.)g\.kr$", r"(^|\.)edu$",
        r"(^|\.)who\.int$", r"(^|\.)un\.org$", r"(^|\.)europe\.eu$",
        r"(^|\.)rfc-editor\.org$", r"(^|\.)ietf\.org$", r"(^|\.)iso\.org$", r"(^|\.)ieee\.org$",
    ],
    # 2í‹°ì–´ â€” ì£¼ìš” ì–¸ë¡ /ë¹…í…Œí¬/í‘œì¤€ ì‚¬ì´íŠ¸
    2: [
        # í•´ì™¸ ì–¸ë¡ 
        r"(^|\.)reuters\.com$", r"(^|\.)apnews\.com$", r"(^|\.)bbc\.com$",
        r"(^|\.)nytimes\.com$", r"(^|\.)wsj\.com$", r"(^|\.)bloomberg\.com$",
        r"(^|\.)theguardian\.com$", r"(^|\.)cnn\.com$", r"(^|\.)cnbc\.com$",
        r"(^|\.)forbes\.com$", r"(^|\.)economist\.com$", r"(^|\.)washingtonpost\.com$",
        # í•œêµ­ ì–¸ë¡ 
        r"(^|\.)hani\.co\.kr$", r"(^|\.)yna\.co\.kr$", r"(^|\.)yonhapnews\.co\.kr$",
        r"(^|\.)kbs\.co\.kr$", r"(^|\.)mbc\.co\.kr$", r"(^|\.)sbs\.co\.kr$",
        r"(^|\.)chosun\.com$", r"(^|\.)joongang\.co\.kr$", r"(^|\.)donga\.com$",
        r"(^|\.)jtbc\.co\.kr$", r"(^|\.)mk\.co\.kr$", r"(^|\.)edaily\.co\.kr$",
        r"(^|\.)koreatimes\.co\.kr$", r"(^|\.)koreaherald\.com$", r"(^|\.)asiatoday\.co\.kr$",
        r"(^|\.)newsis\.co\.kr$", r"(^|\.)heraldcorp\.com$",
        # ê¸°ìˆ /í‘œì¤€/ê¸°ì—…
        r"(^|\.)microsoft\.com$", r"(^|\.)apple\.com$", r"(^|\.)google\.com$",
        r"(^|\.)meta\.com$", r"(^|\.)cloudflare\.com$", r"(^|\.)mozilla\.org$",
        r"(^|\.)oracle\.com$", r"(^|\.)intel\.com$", r"(^|\.)nvidia\.com$",
    ],
}

COMPILED_PATTERNS = {tier: [re.compile(p) for p in patterns] for tier, patterns in TRUST_TIER_PATTERNS.items()}

def classify_domain(domain: str) -> int:
    """ë„ë©”ì¸ ë¬¸ìì—´ì„ 1/2/3 í‹°ì–´ë¡œ ë¶„ë¥˜. ì¼ì¹˜ ì—†ìœ¼ë©´ 1.
    ì •ê·œì‹ì€ ë„ë©”ì¸ ë ìˆ˜ì¤€ì—ì„œ ì¼ì¹˜(ì„œë¸Œë„ë©”ì¸ í¬í•¨)í•˜ë„ë¡ ì„¤ê³„ë¨."""
    d = domain.lower()
    for tier, pats in COMPILED_PATTERNS.items():
        for pat in pats:
            if re.search(pat, d):
                return tier
    return 1

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ë°ì´í„° ëª¨ë¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class Evidence:
    title: str
    url: str
    snippet: str
    domain: str
    trust_tier: int

@dataclass
class ClaimAssessment:
    claim_id: str
    claim_text: str
    normalized_query: str
    evidence: List[Evidence]
    exists_evidence: bool
    source_trust_summary: Dict[str, Any]
    model_verdict: str  # supported|refuted|uncertain
    model_confidence: float  # 0~1
    credibility_score: float  # 0~100

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ (í•œêµ­ì–´)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

EXTRACT_PROMPT_TMPL = Template("""
ë‹¹ì‹ ì€ ì‚¬ì‹¤ê²€ì¦ í¸ì§‘ìì…ë‹ˆë‹¤. ì•„ë˜ ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ "ì‚¬ì‹¤íŒë‹¨" ë¬¸ì¥ë§Œ ë½‘ì•„ ê°„ë‹¨í•œ ì£¼ì¥ í˜•íƒœë¡œ ìš”ì•½í•˜ì„¸ìš”.
ê°€ì¹˜íŒë‹¨(ì¢‹ë‹¤/ë‚˜ì˜ë‹¤/ë°”ëŒì§í•˜ë‹¤ ë“±)ì´ë‚˜ ì˜ê²¬/ì¶”ì¸¡ì€ ì œì™¸í•©ë‹ˆë‹¤.

ë˜í•œ ê° ì£¼ì¥ì— ëŒ€í•´ ì£¼ì œ ì¹´í…Œê³ ë¦¬ë¥¼ ë¶„ë¥˜í•˜ì„¸ìš”.
ì¹´í…Œê³ ë¦¬ í›„ë³´: ["tech","science","policy","health","finance","community","general"]

[ì¶œë ¥ í˜•ì‹(JSON)]
{
    "claims": [
    {"id": "C1", "claim": "ìš”ì•½ ì£¼ì¥ í•œ ë¬¸ì¥", "normalized_query": "ì›¹ê²€ìƒ‰ìš© í•µì‹¬ í‚¤ì›Œë“œ", "category": "tech"}
    ]
}

[ì…ë ¥]
$input_text
""")

EVIDENCE_EVAL_PROMPT_TMPL = Template("""
ë‹¹ì‹ ì€ ì‚¬ì‹¤ê²€ì¦ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ë‹¤ìŒ ì£¼ì¥ì„, ì•„ë˜ ì œê³µëœ ì›¹ ìë£Œ ìš”ì•½ë§Œ ê·¼ê±°ë¡œ í‰ê°€í•˜ì„¸ìš”.  
ê° ê·¼ê±°ê°€ ì£¼ì¥ì„ ì…ì¦í•˜ëŠ”ì§€, ë°˜ë°•í•˜ëŠ”ì§€, ê´€ë ¨ì´ ì—†ëŠ”ì§€ ë¥¼ êµ¬ë¶„í•˜ê³   
ì¢…í•©ì ìœ¼ë¡œ ì „ì²´ íŒì •ì„ ë‚´ë¦¬ì„¸ìš”.  
ë¶ˆí™•ì‹¤í•˜ë‹¤ë©´ "uncertain"ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤. ê³¼ì¥ì´ë‚˜ ì¶”ì¸¡ì€ ê¸ˆì§€ë©ë‹ˆë‹¤.

[ì£¼ì¥]
$claim

[ê·¼ê±° ìš”ì•½]
$evidence_bullets

[ì¶œë ¥(JSON)]
{
    "per_evidence": [
        {"url": "...", "judgement": "supports|refutes|irrelevant", "rationale": "í•œ ì¤„ ê·¼ê±° ì„¤ëª…"}
    ],
    "overall_verdict": "supported|refuted|uncertain",
    "confidence": 0.0
}
""")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# LLM í˜¸ì¶œ ìœ í‹¸
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def llm_json(client: OpenAI, prompt: str, schema_name: str, schema: Dict[str, Any]) -> Dict[str, Any]:
    """Responses APIë¡œ JSON ìŠ¤í‚¤ë§ˆ ê°•ì œ ì¶œë ¥.
    ì‹¤íŒ¨ ì‹œ ë¹ˆ dict ë°˜í™˜(ìƒìœ„ ë‹¨ê³„ì—ì„œ fail-safe ì²˜ë¦¬).
    """
    r = client.responses.create(
        model=MODEL_DEFAULT,
        input=prompt,
        text={
            "format": {
                "type": "json_schema",
                "name": schema_name,
                "schema": schema,
                "strict": True,
            }
        },
        instructions="ê²°ê³¼ëŠ” JSONë§Œ ì¶œë ¥.",
    )
    try:
        return json.loads(r.output_text)
    except Exception:
        logger.warning("JSON íŒŒì‹± ì‹¤íŒ¨ â€” ì›ì‹œ ì¶œë ¥ ë³´ê´€ í•„ìš” ì‹œ client ë¡œê·¸ í™•ì¸")
        return {}

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# SerpAPI ê²€ìƒ‰ ì–´ëŒ‘í„° (ì¹´í…Œê³ ë¦¬ë³„ ì¿¼ë¦¬ êµ¬ì„±)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def search_cse(category: str, query: str, max_results: int = 6, *, time_window: Optional[str] = None) -> List[Dict[str, str]]:
    """
    Google Custom Search JSON API ê¸°ë°˜ ê²€ìƒ‰.
    category âˆˆ {"scholarly","government","news","blogs","community","general"}

    - scholarly : í•™ìˆ  ë„ë©”ì¸ site: í•„í„°ë¡œ ì—ë®¬ë ˆì´ì…˜
    - government: .gov/.go.kr/.edu ë“± ê³µê³µ/êµìœ¡ í•„í„°
    - news      : ì£¼ìš” ë‰´ìŠ¤ ë„ë©”ì¸ ë¬¶ìŒ site: í•„í„°
    - blogs     : ë¸”ë¡œê·¸ ë„ë©”ì¸ ë¬¶ìŒ
    - community : ì»¤ë®¤ë‹ˆí‹°/Q&A ë„ë©”ì¸ ë¬¶ìŒ
    - general   : ì „ì²´ ì›¹ (íŠ¹í—ˆ ë„ë©”ì¸ ì œì™¸)

    time_window: {"d","w","m","y"} â†’ CSE dateRestrict ë¡œ ë§¤í•‘
    """
    import requests

    api_key = os.getenv("GOOGLE_CSE_API_KEY")
    cx = os.getenv("GOOGLE_CSE_CX")
    if not api_key or not cx:
        return []

    # íŠ¹í—ˆ ë„ë©”ì¸ ì œì™¸(ì¿¼ë¦¬ ë³´ê°•)
    common_exclude = " -site:patents.google.com"

    # ë²„í‚·ë³„ ë„ë©”ì¸ ê·¸ë£¹
    SCHOLAR_SITES = [
        "arxiv.org","acm.org","ieee.org","springer.com","sciencedirect.com",
        "nature.com","science.org","pnas.org","cell.com","cambridge.org",
    ]
    NEWS_SITES = [
        # í•´ì™¸ ì£¼ìš”
        "reuters.com","apnews.com","bbc.com","nytimes.com","wsj.com","bloomberg.com",
        "theguardian.com","cnn.com","cnbc.com","economist.com","washingtonpost.com",
        # í•œêµ­ ì£¼ìš”
        "yna.co.kr","yonhapnews.co.kr","kbs.co.kr","mbc.co.kr","sbs.co.kr",
        "chosun.com","joongang.co.kr","donga.com","hani.co.kr","jtbc.co.kr","mk.co.kr","edaily.co.kr",
        "koreaherald.com","koreatimes.co.kr","asiatoday.co.kr","newsis.co.kr","heraldcorp.com",
    ]
    BLOG_SITES = [
        "medium.com","tistory.com","velog.io","dev.to","blogspot.com","hashnode.com","brunch.co.kr","naver.com/blog"
    ]
    COMMUNITY_SITES = [
        "reddit.com","stackoverflow.com","superuser.com","serverfault.com",
        "quora.com","news.ycombinator.com","okky.kr","discord.com/invite"
    ]

    # ì¹´í…Œê³ ë¦¬ë³„ ì¿¼ë¦¬ ê°•í™”
    q = query
    if category == "scholarly":
        filt = " OR ".join(f"site:{d}" for d in SCHOLAR_SITES)
        q = f"{query} ({filt}){common_exclude}"
    elif category == "government":
        filt = "(site:.gov OR site:.go.kr OR site:.g.kr OR site:.edu)"
        q = f"{query} {filt}{common_exclude}"
    elif category == "news":
        filt = " OR ".join(f"site:{d}" for d in NEWS_SITES)
        q = f"{query} ({filt}){common_exclude}"
    elif category == "blogs":
        filt = " OR ".join(f"site:{d}" for d in BLOG_SITES)
        q = f"{query} ({filt}){common_exclude}"
    elif category == "community":
        filt = " OR ".join(f"site:{d}" for d in COMMUNITY_SITES)
        q = f"{query} ({filt}){common_exclude}"
    else:  # general
        q = f"{query}{common_exclude}"

    # CSE íŒŒë¼ë¯¸í„° êµ¬ì„±
    # ì°¸ê³ : https://developers.google.com/custom-search/v1/reference/rest/v1/cse/list
    # gl/hlê³¼ ìœ ì‚¬í•œ íš¨ê³¼ëŠ” lr=lang_ko + ì¿¼ë¦¬/ì—”ì§„ ì„¤ì •ìœ¼ë¡œ ì–´ëŠì •ë„ ìœ ë„ ê°€ëŠ¥
    base_url = "https://www.googleapis.com/customsearch/v1"
    params = {
        "key": api_key,
        "cx": cx,
        "q": q,
        "num": min(max_results, 10),   # CSEëŠ” ìš”ì²­ë‹¹ ìµœëŒ€ 10ê°œ
    }

    # time_window â†’ dateRestrict ë§¤í•‘ (d,w,m,y) â†’ d1, w1, m1, y1
    if time_window in ("d","w","m","y"):
        params["dateRestrict"] = {"d":"d1","w":"w1","m":"m1","y":"y1"}[time_window]

    results: List[Dict[str, str]] = []
    start = 1
    remaining = max_results

    def _request(p) -> dict:
        try:
            r = requests.get(base_url, params=p, timeout=TIMEOUT_S)
            r.raise_for_status()
            return r.json()
        except Exception:
            return {}

    while remaining > 0:
        params["start"] = start
        data = _request(params)
        items = data.get("items", []) if isinstance(data, dict) else []
        if not items:
            break
        for it in items:
            results.append({
                "title": it.get("title", ""),
                "url": it.get("link", ""),
                "snippet": it.get("snippet", "") or it.get("htmlSnippet",""),
            })
            if len(results) >= max_results:
                break
        if len(items) < params["num"]:
            break
        start += params["num"]
        remaining = max_results - len(results)

    # ì¤‘ë³µ URL ì œê±°
    seen, out = set(), []
    for r in results:
        u = r.get("url", "")
        if not u or u in seen:
            continue
        seen.add(u)
        out.append(r)
        if len(out) >= max_results:
            break
    return out

def search_serpapi(category: str, query: str, max_results: int = 6) -> List[Dict[str, str]]:
    """
    category âˆˆ {"scholarly","government","news","blogs","community","general"}
    - scholarly : google_scholar ìš°ì„ , ë¶€ì¡± ì‹œ í•™ìˆ  ë„ë©”ì¸ site: í•„í„°ë¡œ ë³´ê°•
    - government: .gov/.go.kr/.edu ë“± ê³µê³µ/êµìœ¡ ë„ë©”ì¸ ìš°ì„ 
    - news      : êµ¬ê¸€ ë‰´ìŠ¤ íƒ­(tbmn=nws)
    - blogs     : ë¸”ë¡œê·¸ ë„ë©”ì¸ ë¬¶ìŒ í•„í„°
    - community : ì»¤ë®¤ë‹ˆí‹°/Q&A ë„ë©”ì¸ ë¬¶ìŒ í•„í„°
    - general   : ì¼ë°˜ ì›¹ ê²€ìƒ‰(ê¸°ë³¸ì ìœ¼ë¡œ íŠ¹í—ˆ ë„ë©”ì¸ ì œì™¸)
    """
    import requests

    api_key = os.getenv("SERPAPI_API_KEY")
    if not api_key:
        return []

    def _request(params) -> dict:
        try:
            resp = requests.get(SERPAPI_ENDPOINT, params=params, timeout=TIMEOUT_S)
            resp.raise_for_status()
            return resp.json()
        except Exception:
            return {}

    # íŠ¹í—ˆ ë„ë©”ì¸ì€ ê³µí†µì ìœ¼ë¡œ ë°°ì œ (ë…¸ì´ì¦ˆ ë°©ì§€)
    common_exclude = " -site:patents.google.com"

    # ê²€ìƒ‰ ë„ë©”ì¸ ê·¸ë£¹
    SCHOLAR_SITES = [
        "arxiv.org","acm.org","ieee.org","springer.com","sciencedirect.com",
        "nature.com","science.org","pnas.org","cell.com","cambridge.org",
    ]
    BLOG_SITES = [
        "medium.com","tistory.com","velog.io","dev.to","blogspot.com","hashnode.com","brunch.co.kr","naver.com/blog"
    ]
    COMMUNITY_SITES = [
        "reddit.com","stackoverflow.com","superuser.com","serverfault.com",
        "quora.com","news.ycombinator.com","okky.kr", "discord.com/invite"
    ]

    results: List[Dict[str, str]] = []

    # 	1.	scholarly â†’ engine="google_scholar" (êµ¬ê¸€ ìŠ¤ì¹¼ë¼ ì¸ë±ìŠ¤)
	#   2.	news â†’ tbm="nws" (êµ¬ê¸€ ë‰´ìŠ¤ ì¸ë±ìŠ¤)
    if category == "scholarly":
        # 1) êµ¬ê¸€ ìŠ¤ì¹¼ë¼ ìš°ì„ 
        params = {"engine": "google_scholar", "q": query, "api_key": api_key, "hl": SERP_HL}
        data = _request(params)
        for it in data.get("organic_results", [])[:max_results]:
            results.append({
                "title": it.get("title",""),
                "url": it.get("link",""),
                "snippet": it.get("snippet","") or it.get("publication_info",{}).get("summary",""),
            })
        # 2) ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ì›¹ + í•™ìˆ  site í•„í„°ë¡œ ë³´ê°•
        if len(results) < max_results:
            filt = " OR ".join(f"site:{d}" for d in SCHOLAR_SITES)
            params = {
                "engine": "google",
                "q": f"{query} ({filt}){common_exclude}",
                "api_key": api_key,
                "num": max_results - len(results),
                "hl": SERP_HL, "gl": SERP_GL,
            }
            data = _request(params)
            for it in data.get("organic_results", []):
                results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","")})

    elif category == "government":
        # ëŒ€í‘œ ì ‘ë¯¸ì‚¬ ê¸°ë°˜ site í•„í„° (ë‹¨ìˆœ/ì•ˆì „)
        filt = "(site:.gov OR site:.go.kr OR site:.g.kr OR site:.edu)"
        params = {"engine": "google", "q": f"{query} {filt}{common_exclude}", "api_key": api_key, "num": max_results, "hl": SERP_HL, "gl": SERP_GL}
        data = _request(params)
        for it in data.get("organic_results", [])[:max_results]:
            results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","")})

    elif category == "news":
        # êµ¬ê¸€ ë‰´ìŠ¤ íƒ­
        params = {"engine": "google", "tbm": "nws", "q": f"{query}{common_exclude}", "api_key": api_key, "num": max_results, "hl": SERP_HL, "gl": SERP_GL}
        data = _request(params)
        for it in data.get("news_results", [])[:max_results]:
            results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","") or it.get("source","")})

    elif category == "blogs":
        filt = " OR ".join(f"site:{d}" for d in BLOG_SITES)
        params = {"engine": "google", "q": f"{query} ({filt}){common_exclude}", "api_key": api_key, "num": max_results, "hl": SERP_HL, "gl": SERP_GL}
        data = _request(params)
        for it in data.get("organic_results", [])[:max_results]:
            results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","")})

    elif category == "community":
        filt = " OR ".join(f"site:{d}" for d in COMMUNITY_SITES)
        params = {"engine": "google", "q": f"{query} ({filt}){common_exclude}", "api_key": api_key, "num": max_results, "hl": SERP_HL, "gl": SERP_GL}
        data = _request(params)
        for it in data.get("organic_results", [])[:max_results]:
            results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","")})

    else:  # general
        params = {"engine": "google", "q": f"{query}{common_exclude}", "api_key": api_key, "num": max_results, "hl": SERP_HL, "gl": SERP_GL}
        data = _request(params)
        for it in data.get("organic_results", [])[:max_results]:
            results.append({"title": it.get("title",""), "url": it.get("link",""), "snippet": it.get("snippet","")})

    # ì¤‘ë³µ URL ì œê±°(ê°„ë‹¨í•œ seen ì„¸íŠ¸)
    seen, out = set(), []
    for r in results:
        u = r.get("url", "")
        if not u or u in seen:
            continue
        seen.add(u)
        out.append(r)
        if len(out) >= max_results:
            break
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 1 â€” ì£¼ì¥ ì¶”ì¶œ (ì‚¬ì‹¤íŒë‹¨ + ì¹´í…Œê³ ë¦¬)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def step1_extract_claims(client: OpenAI, text: str) -> List[Dict[str, str]]:
    """ì…ë ¥ í…ìŠ¤íŠ¸ì—ì„œ ì‚¬ì‹¤ íŒë‹¨ ë¬¸ì¥ë§Œ ì¶”ì¶œ + ê²€ìƒ‰ìš© ì¿¼ë¦¬/ì¹´í…Œê³ ë¦¬ ë¶€ì—¬.
    ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜.
    """
    schema = {
        "type": "object",
        "properties": {
            "claims": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "id": {"type": "string"},
                        "claim": {"type": "string"},
                        "normalized_query": {"type": "string"},
                        "category": {"type": "string", "enum": FACT_CATS},
                    },
                    "required": ["id", "claim", "normalized_query", "category"],
                    "additionalProperties": False,
                },
            }
        },
        "required": ["claims"],
        "additionalProperties": False,
    }
    prompt = EXTRACT_PROMPT_TMPL.substitute(input_text=text.strip())
    data = llm_json(client, prompt, "ClaimList", schema)
    claims = data.get("claims", []) if isinstance(data, dict) else []

    # ì¤‘ë³µ ì œê±° + í•„ë“œ ë³´ì •
    out, seen_texts = [], set()
    for c in claims:
        t = (c.get("claim", "") or "").strip()
        if t and t not in seen_texts:
            seen_texts.add(t)
            out.append({
                "id": c.get("id", f"C{len(out)+1}"),
                "claim": t,
                "normalized_query": c.get("normalized_query", t),
                "category": c.get("category", "general"),
            })
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 2 â€” ê·¼ê±° ìˆ˜ì§‘(SerpAPI) + ì •ê·œí™”/í‹°ì–´/ë„ë©”ì¸ ë‹¤ì–‘ì„±/ì •ë ¬
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def step2_collect_evidence(
    query: str,
    k: int = MAX_RESULTS,
    categories: Optional[List[str]] = None,   # ì˜ˆ: ["scholarly","government","news"]
    *,
    topic: str = "general",                   # ì£¼ì œ: tech/science/policy/health/finance/general
    locale: str = "KR",                       # ì§€ì—­: "KR", "US" ...
    authority_policy: str = "auto",           # "auto" | "always" | "never"
    authority_extra: Optional[List[str]] = None,  # ì¶”ê°€ë¡œ ê°•ì œ í¬í•¨í•  ë„ë©”ì¸ë“¤
    time_window: Optional[str] = None         # "d"=24h, "w"=7days, "m"=30days, "y"=year (ê°„ë‹¨ í‚¤ì›Œë“œ ë³´ê°•)
) -> List[Evidence]:
    
    """
    ê²€ìƒ‰ ì „ëµ ìš”ì•½:
    1) categoriesê°€ ìˆìœ¼ë©´ ë²„í‚·(í•™ìˆ /ì •ë¶€/ë‰´ìŠ¤/ë¸”ë¡œê·¸/ì»¤ë®¤ë‹ˆí‹°)ë³„ ê²€ìƒ‰
    2) ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰ â†’ (authority_policyì— ë”°ë¼) ê¶Œìœ„ ë„ë©”ì¸ í´ë°± ë‹¨ê³„ì  ì‹œë„
    3) locale/ì£¼ì œì— ë”°ë¼ ê¶Œìœ„ ë„ë©”ì¸ì„ êµ¬ì„±í•˜ê³  í•„ìš” ì‹œ í™•ì¥
    4) íŠ¹í—ˆ ë„ë©”ì¸ ì°¨ë‹¨, URL ì •ê·œí™”/ì¤‘ë³µ ì œê±°, ë„ë©”ì¸ ë‹¤ì–‘ì„± ìœ ì§€, í‹°ì–´ ìš°ì„  ì •ë ¬
    """

    # URL ì •ê·œí™”(utm ë“± ì¶”ì  íŒŒë¼ë¯¸í„° ì œê±°, fragment ì œê±°)
    def _normalize_url(u: str) -> str:
        try:
            p = urlparse.urlparse(u)
            drop = {"utm_source","utm_medium","utm_campaign","utm_term","utm_content","gclid","fbclid","mc_cid","mc_eid"}
            q = [(k,v) for (k,v) in urlparse.parse_qsl(p.query, keep_blank_values=True) if k.lower() not in drop]
            return urlparse.urlunparse((p.scheme, p.netloc.lower(), p.path, "", urlparse.urlencode(q), ""))
        except Exception:
            return u

    def _rows_by_category(cat: str, n: int) -> List[Dict[str, str]]:
        return search_cse(cat, query, max_results=n, time_window=time_window) or []

    def _rows_google(q: str, n: int) -> List[Dict[str, str]]:
        q2 = f"{q} -site:patents.google.com"
        if time_window in ("d","w","m","y"):
            q2 += " "
        return search_cse("general", q2, max_results=n, time_window=time_window) or []

    def _rows_site_sweep(domains: List[str], q: str, per_domain: int) -> List[Dict[str, str]]:
        out: List[Dict[str, str]] = []
        if not domains:
            return out
        chunk = 8
        for i in range(0, len(domains), chunk):
            group = domains[i:i+chunk]
            filt = " OR ".join(f"site:{d}" for d in group)
            q2 = f"{q} ({filt}) -site:patents.google.com"
            out += search_cse("general", q2, max_results=per_domain * len(group), time_window=time_window) or []
            if len(out) >= per_domain * len(domains):
                break
        return out

    # ê¶Œìœ„ ë„ë©”ì¸(ì£¼ì œ/ì§€ì—­ ê¸°ë°˜) êµ¬ì„±
    BASE_AUTHORITY: Dict[str, List[str]] = {
        "tech":    ["ietf.org","rfc-editor.org","w3.org","iana.org","developer.mozilla.org","learn.microsoft.com","cloudflare.com","google.com"],
        "science": ["arxiv.org","nature.com","science.org","springer.com","sciencedirect.com","pnas.org","cell.com","nih.gov"],
        "policy":  ["un.org","oecd.org","reuters.com","apnews.com","bbc.com","nytimes.com"],
        "health":  ["who.int","cdc.gov","fda.gov","ema.europa.eu","nih.gov","bmj.com","thelancet.com","nejm.org"],
        "finance": ["reuters.com","apnews.com","bloomberg.com","wsj.com","ft.com"],
        "general": ["reuters.com","apnews.com","bbc.com","nature.com"],
    }
    LOCALE_AUTHORITY: Dict[str, List[str]] = {
        "KR": ["korea.kr","go.kr","stat.go.kr","yna.co.kr","kbs.co.kr","mbc.co.kr","sbs.co.kr","chosun.com","joongang.co.kr","hani.co.kr"],
        "US": ["cdc.gov","fda.gov","nih.gov","nasa.gov","nytimes.com","wsj.com","apnews.com","reuters.com"],
        "EU": ["ec.europa.eu","ema.europa.eu","who.int","oecd.org","reuters.com","bbc.com"],
    }

    base = BASE_AUTHORITY.get(topic, BASE_AUTHORITY["general"])
    loc = LOCALE_AUTHORITY.get(locale.upper(), [])
    authority_domains = list(dict.fromkeys(base + loc + (authority_extra or [])))

    # í•™ìˆ  ê°•í™” íŒíŠ¸(edu/ac.kr)
    scholarly_boost = ("scholarly" in (categories or [])) or (topic in ("science","health","tech"))
    if scholarly_boost:
        authority_domains = list(dict.fromkeys(authority_domains + ["edu","ac.kr"]))

    # 1) ê²€ìƒ‰ ìˆ˜í–‰
    raw_rows: List[Dict[str, str]] = []

    if categories:  # ë²„í‚·ë³„ ê²€ìƒ‰ ê²½ë¡œ
        per_bucket = max(1, k // len(categories))
        for cat in categories:
            raw_rows += _rows_by_category(cat, per_bucket)
        if len(raw_rows) < k:
            raw_rows += _rows_by_category("general", k - len(raw_rows))
    else:           # ì¼ë°˜ ê²€ìƒ‰ ê²½ë¡œ(+ ê¶Œìœ„ í´ë°±)
        raw_rows += _rows_google(query, k)

        def need_authority_fallback(rows: List[Dict[str,str]]) -> bool:
            # í‹°ì–´2/3 ë„ë©”ì¸ì´ ì¶©ë¶„í•œì§€ í‰ê°€(ì¤‘ë³µ ë„ë©”ì¸ ì œì™¸)
            t23, seen_here = 0, set()
            for r in rows:
                u = r.get("url") or r.get("link") or ""
                if not u:
                    continue
                try:
                    d = urlparse.urlparse(u).netloc
                except Exception:
                    d = ""
                if d in seen_here:
                    continue
                seen_here.add(d)
                if classify_domain(d) >= 2:
                    t23 += 1
            return t23 < 2

        run_fallback = ((authority_policy == "always") or (authority_policy == "auto" and need_authority_fallback(raw_rows)))
        if run_fallback and authority_domains:
            per_dom = 1 if k <= 6 else 2
            raw_rows += _rows_site_sweep(authority_domains, query, per_dom)
            if scholarly_boost and len(raw_rows) < k:
                raw_rows += _rows_google(f'{query} (filetype:pdf OR "white paper")', max(2, k - len(raw_rows)))

    # 2) URL ì •ê·œí™” + ì¤‘ë³µ ì œê±° + íŠ¹í—ˆ ë„ë©”ì¸ ë°©í™”ë²½
    seen, rows = set(), []
    for r in raw_rows:
        u = r.get("url") or r.get("link") or ""
        if not u:
            continue
        nu = _normalize_url(u)
        if nu in seen:
            continue
        if "patents.google.com" in urlparse.urlparse(nu).netloc:
            continue
        seen.add(nu)
        rows.append({"title": r.get("title","") or r.get("name",""), "url": nu, "snippet": r.get("snippet","") or r.get("description","")})

    # 3) Evidence ë³€í™˜ + í‹°ì–´ ë¶€ì—¬
    evs_tmp: List[Evidence] = []
    for r in rows:
        u = r["url"]
        try:
            d = urlparse.urlparse(u).netloc
        except Exception:
            d = ""
        evs_tmp.append(Evidence(title=r["title"], url=u, snippet=r["snippet"], domain=d, trust_tier=classify_domain(d)))

    if not evs_tmp:
        return []

    # 4) ë„ë©”ì¸ ë‹¤ì–‘ì„± ìœ ì§€ + ì •ë ¬(í‹°ì–´ desc â†’ ìŠ¤ë‹ˆí« ê¸¸ì´ desc â†’ ì œëª© ê¸¸ì´ desc)
    def _key(ev: Evidence) -> Tuple[int,int,int]:
        return (ev.trust_tier, len(ev.snippet or ""), len(ev.title or ""))

    buckets: Dict[str, List[Evidence]] = {}
    for e in evs_tmp:
        buckets.setdefault(e.domain, []).append(e)
    for dmn in buckets:
        buckets[dmn].sort(key=_key, reverse=True)

    merged: List[Evidence] = []
    for tier in (3, 2, 1):
        # 1ë¼ìš´ë“œ: ë„ë©”ì¸ë‹¹ 1ê°œ
        for dmn, lst in list(buckets.items()):
            if len(merged) >= k:
                break
            for i, e in enumerate(lst):
                if e.trust_tier == tier:
                    merged.append(lst.pop(i))
                    break
        # 2ë¼ìš´ë“œ: ë‚¨ì€ ìŠ¬ë¡¯ì—ì„œ ë„ë©”ì¸ë‹¹ 1ê°œ ë”
        if len(merged) < k:
            for dmn, lst in list(buckets.items()):
                if len(merged) >= k:
                    break
                for i, e in enumerate(lst):
                    if e.trust_tier == tier:
                        merged.append(lst.pop(i))
                        break

    if len(merged) < k:  # ì—¬ì „íˆ ë¶€ì¡±í•˜ë©´ ë‚˜ë¨¸ì§€ì—ì„œ ì±„ìš°ê¸°
        rest: List[Evidence] = []
        for lst in buckets.values():
            rest.extend(lst)
        rest.sort(key=_key, reverse=True)
        for e in rest:
            if len(merged) >= k:
                break
            merged.append(e)

    return merged[:k]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 3 â€” LLMìœ¼ë¡œ ê·¼ê±°-ì£¼ì¥ ë§¤í•‘ íŒì •(supports/refutes/irrelevant)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def step3_evaluate_sources(client: OpenAI, claim_text: str, evidences: List[Evidence]) -> Dict[str, Any]:
    bullets = [f"- [{ev.domain}] {ev.title} â€” {ev.snippet[:300]} (URL: {ev.url})" for ev in evidences]
    prompt = EVIDENCE_EVAL_PROMPT_TMPL.substitute(
        claim=claim_text,
        evidence_bullets="\n".join(bullets) or "(ê·¼ê±° ì—†ìŒ)",
    )

    schema = {
        "type": "object",
        "properties": {
            "per_evidence": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "url": {"type": "string"},
                        "judgement": {"type": "string", "enum": ["supports", "refutes", "irrelevant"]},
                        "rationale": {"type": "string"},
                    },
                    "required": ["url", "judgement", "rationale"],
                    "additionalProperties": False,
                },
            },
            "overall_verdict": {"type": "string", "enum": ["supported", "refuted", "uncertain"]},
            "confidence": {"type": "number"},
        },
        "required": ["per_evidence", "overall_verdict", "confidence"],
        "additionalProperties": False,
    }

    out = llm_json(client, prompt, "EvidenceEval", schema)
    if not out:
        out = {"per_evidence": [], "overall_verdict": "uncertain", "confidence": 0.0}
    return out

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Step 4 â€” ì ìˆ˜í™”(íœ´ë¦¬ìŠ¤í‹± + íŒì •/í™•ì‹ ë„ â†’ 0~100)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def step4_score(claim_text: str, evidences: List[Evidence], eval_out: Dict[str, Any]) -> ClaimAssessment:
    exists = len(evidences) > 0
    tiers = [ev.trust_tier for ev in evidences]
    tier_counts = {1: tiers.count(1), 2: tiers.count(2), 3: tiers.count(3)}

    per_evi = eval_out.get("per_evidence", [])
    supports = sum(1 for p in per_evi if p.get("judgement") == "supports")
    refutes  = sum(1 for p in per_evi if p.get("judgement") == "refutes")

    verdict = eval_out.get("overall_verdict", "uncertain")
    conf = float(eval_out.get("confidence", 0.0))

    # ì ìˆ˜ ê³„ì‚° ê·œì¹™
    # 1. ê·¼ê±° ì¡´ì¬ ê¸°ë³¸ ì ìˆ˜ + 10ì 
    # 2. í‹°ì–´ ê°€ì¤‘ì¹˜ : 3*10, 2*5, 1*1 
    # 3. ê° ê·¼ê±°ì˜ íŒì •ë³„ ì ìˆ˜ í•©ì‚° : (supports - refutes) * 3
    # 4. ìµœì¢… íŒì • ì ìˆ˜ : supported +10 / refuted -15 (ë°˜ë°•ì€ ê°•í•˜ê²Œ)
    # 5. GPTê°€ ë°˜í™˜í•´ì¤€ í™•ì‹ ë„(0~1) * 40
    score = 0.0
    if exists:
        score += 10
    score += tier_counts[3] * 10 + tier_counts[2] * 5 + tier_counts[1] * 1
    score += max(0, supports - refutes) * 3
    if verdict == "supported":
        score += 10
    elif verdict == "refuted":
        score -= 15
    score += max(0.0, min(conf, 1.0)) * 40
    score = max(0.0, min(score, 100.0))


    return ClaimAssessment(
        claim_id="",
        claim_text=claim_text,
        normalized_query=claim_text,
        evidence=evidences,
        exists_evidence=exists,
        source_trust_summary={"tier_counts": {str(k): v for k, v in tier_counts.items()}},
        model_verdict=verdict,
        model_confidence=round(conf, 3),
        credibility_score=round(score, 1),
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì‹¤í–‰ ë£¨í‹´(íŒŒì´í”„ë¼ì¸) + ì½˜ì†” ë¦¬í¬íŠ¸ ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _process_one_claim(
    client: OpenAI,
    claim: Dict[str, Any],
    idx: int,
) -> Tuple[Dict[str, Any], Dict[str, float]]:
    """ë‹¨ì¼ ì£¼ì¥ì— ëŒ€í•´ Step2~4ë¥¼ ì‹¤í–‰í•˜ê³  (assessment, timings)ë¥¼ ë°˜í™˜"""
    timings: Dict[str, float] = {}

    claim_id = claim.get("id", f"C{idx}")
    ctext = claim.get("claim", "").strip()
    nquery = claim.get("normalized_query", ctext)
    claim_category = claim.get("category", "general")
    cats_for_search = CATEGORY_PRESETS.get(claim_category, CATEGORY_PRESETS["general"])

    # STEP 2 â€” ê·¼ê±° ìˆ˜ì§‘
    t2 = time.perf_counter()
    ev = step2_collect_evidence(
        query=nquery,
        k=MAX_RESULTS,
        categories=cats_for_search,
        topic=claim_category,
        locale="KR",
        authority_policy="auto",
    )
    timings[f"{claim_id}_step2_collect"] = round(time.perf_counter() - t2, 3)

    # STEP 3 â€” ê·¼ê±° íŒì • (GPT í˜¸ì¶œ)
    t3 = time.perf_counter()
    eval_out = step3_evaluate_sources(client, ctext, ev)
    timings[f"{claim_id}_step3_evaluate"] = round(time.perf_counter() - t3, 3)

    # STEP 4 â€” ì ìˆ˜í™”
    t4 = time.perf_counter()
    assess = step4_score(ctext, ev, eval_out)
    assess.claim_id = claim_id
    assess.normalized_query = nquery
    timings[f"{claim_id}_step4_score"] = round(time.perf_counter() - t4, 3)

    return asdict(assess), timings

def run_factchain(text: str, model: Optional[str] = None) -> Dict[str, Any]:
    """ì—”ë“œíˆ¬ì—”ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ â†’ JSON ë¦¬í¬íŠ¸ ë°˜í™˜(ì£¼ì¥ ë‹¨ìœ„ ë³‘ë ¬ ì²˜ë¦¬)."""
    load_dotenv(override=True)
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    if not client.api_key:
        raise RuntimeError("í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")

    global MODEL_DEFAULT
    if model:
        MODEL_DEFAULT = model

    timings: Dict[str, float] = {}
    t0 = time.perf_counter()

    # STEP 1 â€” ì£¼ì¥ ì¶”ì¶œ (ë‹¨ì¼ í˜¸ì¶œ; ì§ë ¬ë¡œ OK)
    t1 = time.perf_counter()
    claims = step1_extract_claims(client, text)
    timings["step1_extract_claims"] = round(time.perf_counter() - t1, 3)

    # ì£¼ì¥ë³„ Step2~4 ë³‘ë ¬ ì²˜ë¦¬
    assessments: List[Dict[str, Any]] = []

    # ë³‘ë ¬ë„ëŠ” í™˜ê²½/ìš”ê¸ˆì œ(RPS, TPM)Â·SerpAPI ì¿¼í„° ê³ ë ¤í•´ì„œ ì ì ˆíˆ ì¡°ì ˆ
    max_workers = min(6, max(1, os.cpu_count() or 4))  # ì˜ˆ: 4~6
    with ThreadPoolExecutor(max_workers=max_workers) as ex:
        futures = []
        for idx, claim in enumerate(claims, start=1):
            futures.append(ex.submit(_process_one_claim, client, claim, idx))

        # ì œì¶œ ìˆœì„œê°€ ì•„ë‹Œ **ì™„ë£Œ ìˆœì„œ**ë¡œ ëª¨ìœ¼ë˜,
        # ì¶œë ¥ì€ ì›ë˜ claim_id ìˆœì„œê°€ í•„ìš”í•˜ë©´ ë‚˜ì¤‘ì— ì •ë ¬
        results = []
        for fut in as_completed(futures):
            try:
                assess, tdict = fut.result()
                results.append((assess["claim_id"], assess, tdict))
            except Exception as e:
                # ì•ˆì „ì¥ì¹˜: ì‹¤íŒ¨í•œ ì£¼ì¥ì€ placeholderë¡œ ê¸°ë¡
                cid = f"CX_{int(time.time()*1000)}"
                results.append((cid, {
                    "claim_id": cid,
                    "claim_text": "<processing_failed>",
                    "normalized_query": "",
                    "evidence": [],
                    "exists_evidence": False,
                    "source_trust_summary": {"tier_counts": {"1":0,"2":0,"3":0}},
                    "model_verdict": "uncertain",
                    "model_confidence": 0.0,
                    "credibility_score": 0.0,
                }, {f"{cid}_error": 0.0}))
                logger.exception("claim ë³‘ë ¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: %s", e)

    # ì›ë˜ ìˆœì„œë¡œ ì •ë ¬(Claim IDê°€ C1, C2â€¦ í˜•íƒœë¼ëŠ” ê°€ì •)
    results.sort(key=lambda x: (
        int(x[0][1:]) if x[0].startswith("C") and x[0][1:].isdigit() else 10**9
    ))
    for _, assess, tdict in results:
        assessments.append(assess)
        timings.update(tdict)

    elapsed = round(time.perf_counter() - t0, 3)
    return {
        "meta": {
            "model": MODEL_DEFAULT,
            "search_provider": "Google Custom Search API",
            "max_results": MAX_RESULTS,
            "elapsed_sec": elapsed,
            "hl": SERP_HL,
            "gl": SERP_GL,
            "timings": timings,
            "parallel": {"enabled": True, "max_workers": max_workers},
        },
        "claims": assessments,
    }

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì½˜ì†” ì¶œë ¥ ìœ í‹¸(ìƒ‰ìƒ/ë¼ë²¨)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def paint(s: str, c: str) -> str:
    return (COLORS.get(c, "") + s + COLORS["reset"]) if USE_COLOR else s

def verdict_label(v: str) -> str:
    mapping = {
        "supported": ("âœ… ì‚¬ì‹¤ë¡œ íŒë‹¨", "green"),
        "refuted": ("âŒ ì‚¬ì‹¤ ì•„ë‹˜", "red"),
        "uncertain": ("âš ï¸ ë¶ˆí™•ì‹¤", "yellow"),
    }
    text, color = mapping.get(v, (v, "yellow"))
    return paint(text, color)

def tc_get(tc: dict, k: int) -> int:
    # int í‚¤/ë¬¸ìì—´ í‚¤ ëª¨ë‘ ëŒ€ì‘
    return tc.get(k, tc.get(str(k), 0))

def tier_icons(tc: dict) -> str:
    t3, t2, t1 = tc_get(tc, 3), tc_get(tc, 2), tc_get(tc, 1)
    parts = [paint(f"ğŸŸ¢3:{t3}", "green"), paint(f"ğŸŸ¡2:{t2}", "yellow"), paint(f"ğŸ”´1:{t1}", "red")]
    return "ì¶œì²˜ ì‹ ë¢°ë„: " + ", ".join(parts)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ â€” CLI + ë°ëª¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

DEMO_TEXT = """
ì•„ì´ìŠˆíƒ€ì¸ì€ 1905ë…„ íŠ¹ìˆ˜ìƒëŒ€ì„±ì´ë¡ ì„ ë°œí‘œí•˜ì—¬ ì‹œê°„ê³¼ ê³µê°„ì˜ ê°œë…ì„ í˜ì‹ ì ìœ¼ë¡œ ë°”ê¾¸ì—ˆë‹¤.  
ì´ ì´ë¡ ì— ë”°ë¥´ë©´ ë¹›ì˜ ì†ë„ëŠ” ê´€ì„±ê³„ì— ê´€ê³„ì—†ì´ ì¼ì •í•˜ë©°,  
ì‹œê°„ì€ ì ˆëŒ€ì ì´ ì•„ë‹ˆë¼ ê´€ì¸¡ìì— ë”°ë¼ ìƒëŒ€ì ìœ¼ë¡œ ë‹¬ë¼ì§„ë‹¤.  
ì´í›„ 1915ë…„ ê·¸ëŠ” ì¼ë°˜ìƒëŒ€ì„±ì´ë¡ ì„ í†µí•´ ì¤‘ë ¥ì´ ì‹œê³µê°„ì˜ ê³¡ë¥ ë¡œ ì„¤ëª…ë  ìˆ˜ ìˆìŒì„ ì œì‹œí–ˆë‹¤.
"""

"""
1) HTTP/3ëŠ” TCPê°€ ì•„ë‹ˆë¼ QUIC ìœ„ì—ì„œ ë™ì‘í•œë‹¤.
2) íƒœì–‘ì€ ì§€êµ¬ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ëˆë‹¤.
3) ë¹„íŠ¸ì½”ì¸ì€ 2009ë…„ì— ì²˜ìŒ ë°œí–‰ë˜ì—ˆë‹¤.
4) AIê°€ ìƒì„±í•œ ê·¸ë¦¼ì€ ì €ì‘ê¶Œ ë³´í˜¸ë¥¼ ë°›ì„ ìˆ˜ ìˆë‹¤.
5) í•œêµ­ì€ 2022ë…„ FIFA ì›”ë“œì»µì—ì„œ ìš°ìŠ¹í–ˆë‹¤.
6) ìˆ˜ì€ì€ ìƒì˜¨ì—ì„œ ì•¡ì²´ ìƒíƒœì¸ ìœ ì¼í•œ ê¸ˆì†ì´ë‹¤.
"""
def _phase_time(timings: dict, suffix: str) -> float:
    vals = [v for k, v in timings.items() if k.endswith(suffix)]
    return round(max(vals) if vals else 0.0, 3)

def main(argv: Optional[List[str]] = None) -> int:
    parser = argparse.ArgumentParser(description="ì‚¬ì‹¤ ê²€ì¦ íŒŒì´í”„ë¼ì¸")
    parser.add_argument("--text", type=str, default=None, help="ì§ì ‘ ì…ë ¥ í…ìŠ¤íŠ¸")
    parser.add_argument("--model", type=str, default=None, help="ì‚¬ìš©í•  OpenAI ëª¨ë¸")
    args = parser.parse_args(argv)

    text = args.text or DEMO_TEXT
    report = run_factchain(text, model=args.model)

    # JSON ì›ë³¸ ì €ì¥(í˜‘ì—…/ë””ë²„ê¹…ìš©)
    with open("output.json", "w", encoding="utf-8") as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    # ì½˜ì†” ìš”ì•½ ì¶œë ¥
    global USE_COLOR, COLORS
    USE_COLOR = True
    COLORS = {
        "green": "\033[92m",
        "yellow": "\033[93m",
        "red": "\033[91m",
        "cyan": "\033[96m",
        "bold": "\033[1m",
        "reset": "\033[0m",
    }

    meta = report.get("meta", {})
    claims = report.get("claims", [])
    timings = meta.get("timings", {})
    step1_time = timings.get("step1_extract_claims", 0.0)
    step2_time = _phase_time(timings, "_step2_collect") 
    step3_time = _phase_time(timings, "_step3_evaluate")  
    step4_time = _phase_time(timings, "_step4_score")

    print("\n[ê²€ì¦ ê²°ê³¼]")
    print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")
    print(f"ëª¨ë¸: {meta.get('model')} | ê²€ìƒ‰ì—”ì§„: {meta.get('search_provider')}")
    print(f"ì‹¤í–‰ì‹œê°„: {meta.get('elapsed_sec')}ì´ˆ | ì£¼ì¥ ìˆ˜: {len(claims)}ê°œ\n")
    print(f"í”„ë¡œì„¸ìŠ¤ë³„ ì†Œìš”ì‹œê°„:\nSTEP 1 (ì£¼ì¥ ì¶”ì¶œ): {step1_time:.2f}ì´ˆ\n"
            f"STEP 2 (ê·¼ê±° ìˆ˜ì§‘): {step2_time:.2f}ì´ˆ\n"
            f"STEP 3 (ê·¼ê±° í‰ê°€): {step3_time:.2f}ì´ˆ\n"
            f"STEP 4 (ì ìˆ˜í™”)  : {step4_time:.2f}ì´ˆ\n")

    for c in claims:
        evid = c.get("evidence", [])
        tc = c.get("source_trust_summary", {}).get("tier_counts", {})
        print(paint(f"ğŸ”¹ [{c.get('claim_id')}] {c.get('claim_text')}", "cyan"))
        print(f"   â†’ íŒì •: {verdict_label(c.get('model_verdict',''))} "
        f"({int(float(c.get('model_confidence', 0))*100)}% í™•ì‹ )")
        print(f"   â†’ ì‹ ë¢°ì ìˆ˜: {c.get('credibility_score')}")
        print(f"   â†’ {tier_icons(tc)}")
        for ev in evid[:3]:
            dom = ev.get('domain') or ''
            title = (ev.get('title') or '')[:70]
            print(f"      - {dom}: {title}...")
        print("â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€")

    print("\n*ì „ì²´ JSONì€ output.json ì— ì €ì¥")
    return 0

if __name__ == "__main__":
    raise SystemExit(main())